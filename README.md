# Vision-Transformer (ViT)
ViT is a deep learning model that applies the Transformer architecture (originally designed for NLP) to image classification. It splits an image into small patches, embeds them, and processes them using self-attention. Unlike CNNs, ViTs rely on global context rather than local features.

I have adapt this github repo code on transformer: https://github.com/jeonsworld/ViT-pytorch
for 2-way classification task (i.e. classification on bees versus ants) on this dataset: https://drive.google.com/file/d/1WdZIp6oTyB9Ug7ZkttsXcC0mtpquZ0_x/view?usp=sharing
